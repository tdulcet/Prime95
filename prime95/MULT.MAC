; Copyright 1998-2000 - Just For Fun Software, Inc., all rights reserved
; Author:  George Woltman
; Email: woltman@alum.mit.edu
;
; These macros take the basic FFT building blocks and build even
; larger building blocks.
;

; Macros to create different labels from the same source code.  This
; allows us to use the same source to create Pentium and Pentium Pro
; specific versions of the code.

EXTRNP	MACRO x
IFDEF PFETCH
EXTRN	&x&P3:PROC
ELSE
IFDEF PPRO
EXTRN	&x&PPRO:PROC
ELSE
EXTRN	&x:PROC
ENDIF
ENDIF
	ENDM

PUBLICP	MACRO x
IFDEF PFETCH
PUBLIC	&x&P3
ELSE
IFDEF PPRO
PUBLIC	&x&PPRO
ELSE
PUBLIC	&x
ENDIF
ENDIF
	ENDM

PROCP	MACRO x
IFDEF PFETCH
&x&P3 PROC NEAR
ELSE
IFDEF PPRO
&x&PPRO PROC NEAR
ELSE
&x	PROC NEAR
ENDIF
ENDIF
	ENDM

ENDPP	MACRO x
IFDEF PFETCH
&x&P3 ENDP
ELSE
IFDEF PPRO
&x&PPRO ENDP
ELSE
&x	ENDP
ENDIF
ENDIF
	ENDM

LABELP	MACRO x
IFDEF PFETCH
&x&P3:
ELSE
IFDEF PPRO
&x&PPRO:
ELSE
&x:
ENDIF
ENDIF
	ENDM

CALLP	MACRO x
IFDEF PFETCH
	CALL_Y	&x&P3
ELSE
IFDEF PPRO
	CALL_Y	&x&PPRO
ELSE
	CALL_Y	&x
ENDIF
ENDIF
	ENDM

JMPP	MACRO x
IFDEF PFETCH
	JMP_Y	&x&P3
ELSE
IFDEF PPRO
	JMP_Y	&x&PPRO
ELSE
	JMP_Y	&x
ENDIF
ENDIF
	ENDM


; *************** macros to expand 4 types of fft macros ******************

;;
;; Generate the 4 types of FFTs for the given run length.
;; The 4 types are:
;;	1) forward FFT,
;;	2) square (forward FFT, multiply, inverse FFT)
;;	3) multiply (forward FFT src1, multiply by src2, inverse FFT)
;;	4) multiply (multiply src1 by src2, inverse FFT)
;;

fft	MACRO fft_length, p
	type1	fft_length, p, _1
	type2	fft_length, p, _2
	type3	fft_length, p, _3
	type4	fft_length, p, _4
	purge	fft&fft_length&p
	ENDM

;;
;; Perform an FFT in preparation for a later multiply
;; Do the forward FFT
;;

type1	MACRO fft_length, p, suffix
	PUBLICP	fft&fft_length&p&suffix
LABELP	fft&fft_length&p&suffix
	fninit
	push	ebp
	push	ebx
	push	edi
	push	esi
	mov	esi, _DESTARG
	mov	ebx, _SRCARG
	sub	ebx, esi
	IF EXPANDING EQ 2
	mov	ffttype, 1
	ENDIF
fft1a&fft_length&p:
	fft&fft_length&p 1
	IF EXPANDING NE 2
	pop	esi			;; Pop values and return
	pop	edi
	pop	ebx
	pop	ebp
	ret
	ENDIF
	ENDM

;;
;; Square a number mod 2**p-1
;; Do the forward FFT, squaring, and inverse FFT
;;

type2	MACRO fft_length, p, suffix
	PUBLICP	fft&fft_length&p&suffix
LABELP	fft&fft_length&p&suffix
	fninit
	push	ebp
	push	ebx
	push	edi
	push	esi
	mov	esi, _DESTARG
	sub	ebx, ebx
	IF EXPANDING EQ 0
	sub	ebp, ebp
	push	OFFSET fft4a&fft_length&p
	push	ebp
	push	ebx
	push	edi
	push	esi
	JMP_X	fft1a&fft_length&p
	ENDIF
	IF EXPANDING EQ 1
	fft&fft_length&p 2
	ENDIF
	IF EXPANDING EQ 2
	mov	ffttype, 2
	push	OFFSET end&fft_length&p
	JMP_X	fft1a&fft_length&p
	ENDIF
	ENDM

;;
;; Multiply two numbers mod 2**p-1.  One of the input numbers (SRCARG) must
;; have already been passed through gw_fft.
;; Do the forward FFT, multiply, and inverse FFT
;;

type3	MACRO fft_length, p, suffix
	PUBLICP	fft&fft_length&p&suffix
LABELP	fft&fft_length&p&suffix
	fninit
	push	ebp
	push	ebx
	push	edi
	push	esi
	mov	esi, _DESTARG
	mov	ebp, _SRCARG
	sub	ebx, ebx
	sub	ebp, esi
	IF EXPANDING EQ 0
	push	OFFSET fft4a&fft_length&p
	push	ebp
	push	ebx
	push	edi
	push	esi
	JMP_X	fft1a&fft_length&p
	ENDIF
	IF EXPANDING EQ 1
	fft&fft_length&p 3
	ENDIF
	IF EXPANDING EQ 2
	mov	ffttype, 3
	push	OFFSET end&fft_length&p
	JMP_X	fft1a&fft_length&p
	ENDIF
	ENDM

;;
;; Multiply two numbers mod 2**p-1.  Both of the input numbers must
;; have already been passed through gw_fft.
;; Do the multiply and inverse FFT
;;

type4	MACRO fft_length, p, suffix
	PUBLICP	fft&fft_length&p&suffix
LABELP	fft&fft_length&p&suffix
	fninit
	push	ebp
	push	ebx
	push	edi
	push	esi
	mov	esi, _DESTARG
	mov	ebp, _SRCARG
	mov	ebx, _SRC2ARG
	sub	ebp, esi
	sub	ebx, esi
fft4a&fft_length&p:
	fft&fft_length&p 4
	ENDM


; ********************************************************
; ********************************************************
; **************** NEW BUILDING BLOCKS  ******************
; ********************************************************
; ********************************************************

; Return from a type 3 FFT - either jump to the common
; error check or normalization code

fft_3_ret MACRO
	mov	eax, _NORMRTN
	jmp	eax
	ENDM

; A funny eight reals that does three FFT levels on the first
; four values and two FFT levels on the last four values.

eight_reals_funny_fft MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1			;; R1
	fadd	R5			;; new R1 = R1 + R5
	fld	R3			;; R3
	fadd	R7			;; new R3 = R3 + R7
	fld	R2			;; R2
	fadd	R6			;; new R2 = R2 + R6
	fld	R4			;; R4
	fadd	R8			;; new R4 = R4 + R8
	 fxch	st(2)			;; R3,R2,R4,R1
	 fsub	st(3), st		;; R1 = R1 - R3 (new R3)
	 fadd	st, st			;; R3 = R3 * 2
	 fxch	st(2)			;; R4,R2,R3,R1
	 fsub	st(1), st		;; R2 = R2 - R4 (new and final R4)
	 fadd	st, st			;; R4 = R4 * 2
	 fxch	st(2)			;; R3,R2,R4,R1
	 fadd	st, st(3)		;; R3 = R1 + R3 (new R1)
	fld	R2			;; R2
	fsub	R6			;; new R6 = R2 - R6
	 fxch	st(2)			;; R2,R3,R6,R4,R1
	 fadd	st(3), st		;; R4 = R2 + R4 (new R2)
	fld	R4			;; R4
	fsub	R8			;; new R8 = R4 - R8
					;; R8,R4,R1,R6,R2,R3
	  fxch	st(4)			;; R2,R4,R1,R6,R8,R3
	  fsub	st(2), st		;; R1 = R1 - R2 (final R2)
	  fadd	st, st			;; R2 = R2 * 2
	fld	R3			;; R3
	fsub	R7			;; new R7 = R3 - R7
	  fxch	st(3)			;; R1,R2,R4,R7,R6,R8,R3
	  fadd	st(1), st		;; R2 = R1 + R2 (final R1)
	fld	R1			;; R1
	fsub	R5			;; new R5 = R1 - R5
	  				;; R5,R1,R2,R4,R7,R6,R8,R3
	 				;; R5,R2,R1,R4,R7,R6,R8,R3
	fxch	st(6)			;; R8,R2,R1,R4,R7,R6,R5,R3
	fstp	R8
	fstp	R2
	fstp	R1
	fstp	R4
	fstp	R7
	fstp	R6
	fstp	R5
	fstp	R3
	ENDM

eight_reals_funny_unfft MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1			;; R1
	fadd	R2			;; new R1 = R1 + R2
	fld	R1			;; R1
	fsub	R2			;; new R2 = R1 - R2
	 fld	st(1)
	 fsub	R3			;; new R3 = R1 - R3
	 fld	st(1)
	 fsub	R4			;; new R4 = R2 - R4
	 fxch	st(3)			;; R1,R3,R2,R4
	 fadd	R3			;; new R1 = R1 + R3
	 fxch	st(2)			;; R2,R3,R1,R4
	 fadd	R4			;; new R2 = R2 + R4
	fld	st(1)
	fsub	R7			;; new R7 = R3 - R7
	fxch	st(2)			;; R3,R2,R7,R1,R4
	fadd	R7			;; new R3 = R3 + R7
	fld	st(4)
	fsub	R8			;; new R8 = R4 - R8
	fxch	st(5)			;; R4,R3,R2,R7,R1,R8
	fadd	R8			;; new R4 = R4 + R8
	fld	st(4)
	fsub	R5			;; new R5 = R1 - R5
	fxch	st(5)			;; R1,R4,R3,R2,R7,R5,R8
	fadd	R5			;; new R1 = R1 + R5
	fld	st(3)
	fsub	R6			;; new R6 = R2 - R6
	fxch	st(4)			;; R2,R1,R4,R3,R6,R7,R5,R8
	fadd	R6			;; new R2 = R2 + R6
	fxch	st(3)			;; R3,R1,R4,R2,R6,R7,R5,R8
	fstp	R3
	fstp	R1
	fstp	R4
	fstp	R2
	fstp	R6
	fstp	R7
	fstp	R5
	fstp	R8
	ENDM

; Variants of building blocks in lucas1.mac

eight_reals_with_mult_1 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R6			;; R6
	fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
	fld	R8			;; R8,R6
	fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	fsub	st(1), st		;; R6 = R6 - R8 (Real part)
	fadd	st, st			;; R8 = R8 * 2
	fadd	st, st(1)		;; R8 = R6 + R8 (Imaginary part)
	fld	R5			;; R5,R8,R6
	fxch	st(2)			;; R6,R8,R5
	fsub	st(2), st		;; R5 = R5 - R6 (Real part - final R7)
	fadd	st, st			;; R6 = R6 * 2
	fld	R1			;; R1,R6,R8,R5
	fld	R2			;; R2,R1,R6,R8,R5
	fsub	st(1), st		;; R1 = R1 - R2 (final R2)
	fadd	st, st			;; R2 = R2 * 2
	fld	R7			;; R7,R2,R1,R6,R8,R5
	fxch	st(4)			;; R8,R2,R1,R6,R7,R5
	fsub	st(4), st		;; R7 = R7 - R8 (Imaginary - final R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(3)			;; R6,R2,R1,R8,R7,R5
	fadd	st, st(5)		;; R6 = R5 + R6 (Real part - final R5)
	fxch	st(1)			;; R2,R6,R1,R8,R7,R5
	fadd	st, st(2)		;; R2 = R1 + R2 (final R1)
	fxch	st(3)			;; R8,R6,R1,R2,R7,R5
	fadd	st, st(4)		;; R8 = R7 + R8 (Imaginary - final R6)
	  				;; R6,R5,R2,R1,R8,R7

	fstp	R6
	fstp	R5
	fstp	R2
	fstp	R1
	fstp	R8
	fstp	R7

	eight_reals_with_mulf_1 R1,R2,R3,R4,R5,R6,R7,R8
	ENDM

eight_reals_with_mult_2 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1			;; R1
	fld	R3			;; R3,R1
	fsub	st(1), st		;; R1 = R1 - R3 (new R3)
	fadd	st, st			;; R3 = R3 * 2
	fld	R6			;; R6,R3,R1
	fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
	fxch	st(1)			;; R3,R6,R1
	fadd	st, st(2)		;; R3 = R1 + R3 (new R1)
	fld	R8			;; R8,R3,R6,R1
	fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	fld	R2			;; R2,R8,R3,R6,R1
	fld	R4			;; R4,R2,R8,R3,R6,R1
	fsub	st(1), st		;; R2 = R2 - R4 (new and final R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(2)			;; R8,R2,R4,R3,R6,R1
	fsub	st(4), st		;; R6 = R6 - R8 (Real part)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(2)			;; R4,R2,R8,R3,R6,R1
	fadd	st, st(1)		;; R4 = R2 + R4 (new R2)
					;; R2,R4,R8,R1,R6,R3
	fld	R5			;; R5,R2,R4,R8,R1,R6,R3
	fxch	st(5)			;; R6,R2,R4,R8,R1,R5,R3
	fadd	st(3), st		;; R8 = R6 + R8 (Imaginary part)
	fsub	st(5), st		;; R5 = R5 - R6 (Real part - final R7)
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(1)			;; R2,R6,R4,R8,R1,R5,R3
	fsub	st(4), st		;; R1 = R1 - R2 (final R2)
	fadd	st, st			;; R2 = R2 * 2
	fld	R7			;; R7,R2,R6,R4,R8,R1,R5,R3
	fxch	st(4)			;; R8,R2,R6,R4,R7,R1,R5,R3
	fsub	st(4), st		;; R7 = R7 - R8 (Imaginary - final R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(2)			;; R6,R2,R8,R4,R7,R1,R5,R3
	fadd	st, st(6)		;; R6 = R5 + R6 (Real part - final R5)
	fxch	st(1)			;; R2,R6,R8,R4,R7,R1,R5,R3
	fadd	st, st(5)		;; R2 = R1 + R2 (final R1)
	fxch	st(2)			;; R8,R6,R2,R4,R7,R1,R5,R3
	fadd	st, st(4)		;; R8 = R7 + R8 (Imaginary - final R6)
	  				;; R6,R5,R1,R4,R8,R2,R7,R3

	fstp	R6
	fstp	R5
	fstp	R1
	fstp	R4
	fstp	R8
	fstp	R2
	fstp	R7
	fstp	R3

	eight_reals_with_mulf_2 R1,R2,R3,R4,R5,R6,R7,R8
	ENDM

eight_reals_with_mult_3 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1			;; R1
	fld	R5			;; R5
	fsub	st(1), st		;; R1 = R1 - R5 (new R5)
	fadd	st, st			;; R5 = R5 * 2
	fld	R3			;; R3
	fld	R7			;; R7
	fsub	st(1), st		;; R3 = R3 - R7 (new R7)
	fadd	st, st			;; R7 = R7 * 2
	fxch	st(2)			;; R5,R3,R7,R1
	fadd	st, st(3)		;; R5 = R1 + R5 (new R1)
	fld	R2			;; R2
	fld	R6			;; R6
	fsub	st(1), st		;; R2 = R2 - R6 (new R6)
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(4)			;; R7,R2,R5,R3,R6,R1
	fadd	st, st(3)		;; R7 = R3 + R7 (new R3)
	fld	R4			;; R4
	fld	R8			;; R8
	fsub	st(1), st		;; R4 = R4 - R8 (new R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(6)			;; R6,R4,R7,R2,R5,R3,R8,R1
	fadd	st, st(3)		;; R6 = R2 + R6 (new R2)
	 fxch	st(2)			;; Round #2 - R3,R8,R2,R6,R1,R7,R4,R5
	 fsub	st(4), st		;; R1 = R1 - R3 (new R3)
	 fadd	st, st			;; R3 = R3 * 2
	fxch	st(6)			;; Round #1 - R8,R4,R6,R2,R5,R3,R7,R1
	fadd	st, st(1)		;; R8 = R4 + R8 (new R4)
	  fxch	st(3)			;; Round #2 - R6,R8,R2,R4,R1,R7,R3,R5
	  fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
	 fxch	st(6)			;; Round #2 - R3,R8,R2,R4,R1,R7,R6,R5
	 fadd	st, st(4)		;; R3 = R1 + R3 (new R1)
	  fxch	st(1)			;; Round #2 - R8,R3,R2,R4,R1,R7,R6,R5
	  fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	 fxch	st(3)			;; Round #2 - R4,R3,R2,R8,R1,R7,R6,R5
	 fsub	st(2), st		;; R2 = R2 - R4 (new and final R4)
	 fadd	st, st			;; R4 = R4 * 2
	  fxch	st(3)			;; Round #2 - R8,R3,R2,R4,R1,R7,R6,R5
	  fsub	st(6), st		;; R6 = R6 - R8 (Real part)
	  fadd	st, st			;; R8 = R8 * 2
	 fxch	st(2)			;; Round #2 - R2,R3,R8,R4,R1,R7,R6,R5
	 fadd	st(3), st		;; R4 = R2 + R4 (new R2)
	  fxch	st(6)			;; Round #3 - R6,R1,R8,R2,R3,R7,R4,R5
	  fsub	st(7), st		;; R5 = R5 - R6 (Real part - final R7)
	  fadd	st(2), st		;; R8 = R6 + R8 (Imaginary part)
	  fadd	st, st			;; R6 = R6 * 2
	  fxch	st(3)			;; Round #3 - R2,R1,R8,R6,R3,R7,R4,R5
	  fsub	st(1), st		;; R1 = R1 - R2 (final R2)
	  fadd	st, st			;; R2 = R2 * 2
	  fxch	st(2)			;; Round #3 - R8,R1,R2,R6,R3,R7,R4,R5
	  fsub	st(5), st		;; R7 = R7 - R8 (Imaginary - final R8)
	  fadd	st, st			;; R8 = R8 * 2
	  fxch	st(7)			;; Round #3 - R5,R1,R2,R6,R3,R7,R4,R8
	  fadd	st(3), st		;; R6 = R5 + R6 (Real part - final R5)
	  fxch	st(1)			;; Round #3 - R1,R5,R2,R6,R3,R7,R4,R8
	  fadd	st(2), st		;; R2 = R1 + R2 (final R1)
	  fxch	st(5)			;; Round #3 - R7,R5,R2,R6,R3,R1,R4,R8
	  fadd	st(7), st		;; R8 = R7 + R8 (Imaginary - final R6)
	  				;; Final - R8,R7,R1,R5,R3,R2,R4,R6

	fstp	R8
	fstp	R7
	fstp	R1
	fstp	R5
	fstp	R3
	fstp	R2
	fstp	R4
	fstp	R6

	eight_reals_with_mulf_3 R1,R2,R3,R4,R5,R6,R7,R8
	ENDM

eight_reals_with_mulf_1 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R3[ebx]
	fmul	R3[ebp]			;; R33
	fld	R4[ebx]
	fmul	R4[ebp]			;; R44
	fld	R3[ebx]
	fmul	R4[ebp]			;; R34
	fld	R4[ebx]
	fmul	R3[ebp]			;; R43
	fxch	st(2)			;; R44,R34,R43,R33
	fsubp	st(3), st		;; R34,R43,R3
	fld	R5[ebx]
	fmul	R5[ebp]			;; R55,R34,R43,R3
	fxch	st(1)			;; R34,R55,R43,R3
	faddp	st(2), st		;; R55,R4,R3
	fld	R6[ebx]
	fmul	R6[ebp]			;; R66,R55,R4,R3
	fld	R5[ebx]
	fmul	R6[ebp]			;; R56,R66,R55,R4,R3
	fld	R6[ebx]
	fmul	R5[ebp]			;; R65,R56,R66,R55,R4,R3
	fxch	st(2)			;; R66,R56,R65,R55,R4,R3
	fsubp	st(3), st		;; R56,R65,R5,R4,R3
	fld	R7[ebx]
	fmul	R7[ebp]			;; R77,R56,R65,R5,R4,R3
	fxch	st(1)			;; R56,R77,R65,R5,R4,R3
	faddp	st(2), st		;; R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R8[ebp]			;; R88,R77,R6,R5,R4,R3
	fld	R7[ebx]
	fmul	R8[ebp]			;; R78,R88,R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R7[ebp]			;; R87,R78,R88,R77,R6,R5,R4,R3
	fxch	st(2)			;; R88,R78,R87,R77,R6,R5,R4,R3
	fsubp	st(3), st		;; R78,R87,R7,R6,R5,R4,R3
	fld	R1[ebx]
	fmul	R1[ebp]			;; R1,R78,R87,R7,R6,R5,R4,R3
	fxch	st(1)			;; R78,R1,R87,R7,R6,R5,R4,R3
	faddp	st(2), st		;; R1,R8,R7,R6,R5,R4,R3
	fld	R2[ebx]
	fmul	R2[ebp]			;; R2,R1,R8,R7,R6,R5,R4,R3
	fxch	st(3)			;; R7,R1,R8,R2,R6,R5,R4,R3
	fsub	st(5), st		;; R5 = R5 - R7 (new R6)
	fadd	st, st			;; R7 = R7 * 2
	fxch	st(2)			;; R8,R1,R7,R2,R6,R5,R4,R3
	fsub	st(4), st		;; R6 = R6 - R8 (new R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(5)			;; R5,R1,R7,R2,R6,R8,R4,R3
	fadd	st(2), st		;; R7 = R5 + R7 (new R5)
	fxch	st(1)			;; R1,R5,R7,R2,R6,R8,R4,R3
	fst	QWORD PTR [esi-16]	;; Save product of sum of FFT values
	fsub	st, st(3)		;; R1 = R1 - R2 (new R2)
	fxch	st(4)			;; R6,R5,R7,R2,R1,R8,R4,R3
	fadd	st(5), st		;; R8 = R6 + R8 (new R7)
					;; R8,R6,R5,R2,R1,R7,R4,R3
	fxch	st(1)			;; R6,R8,R5,R2,R1,R7,R4,R3
	fsub	st(1), st		;; R8 = R8 - R6
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(4)			;; R1,R8,R5,R2,R6,R7,R4,R3
	fmul	HALF			;; R1 = R1 * HALF
					;; STALL
	fxch	st(1)			;; R8,R1,R5,R2,R6,R7,R4,R3
	fadd	st(4), st		;; R6 = R6 + R8
	fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	fxch	st(1)			;; R1,R8,R5,R2,R6,R7,R4,R3
	fadd	st(3), st		;; R2 = R1 + R2 (new R1)
	fxch	st(4)			;; R6,R8,R5,R2,R1,R7,R4,R3
	fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
					;; R6,R8,R5,R1,R2,R7,R4,R3
	fxch	st(7)			;; R3,R8,R5,R1,R2,R7,R4,R6
	fstp	R3
	fstp	R8
	fstp	R5
	fstp	R1
	fstp	R2
	fstp	R7
	fstp	R4
	fstp	R6
	ENDM

eight_reals_with_mulf_2 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R3[ebx]
	fmul	R3[ebp]			;; R33
	fld	R4[ebx]
	fmul	R4[ebp]			;; R44
	fld	R3[ebx]
	fmul	R4[ebp]			;; R34
	fld	R4[ebx]
	fmul	R3[ebp]			;; R43
	fxch	st(2)			;; R44,R34,R43,R33
	fsubp	st(3), st		;; R34,R43,R3
	fld	R5[ebx]
	fmul	R5[ebp]			;; R55,R34,R43,R3
	fxch	st(1)			;; R34,R55,R43,R3
	faddp	st(2), st		;; R55,R4,R3
	fld	R6[ebx]
	fmul	R6[ebp]			;; R66,R55,R4,R3
	fld	R5[ebx]
	fmul	R6[ebp]			;; R56,R66,R55,R4,R3
	fld	R6[ebx]
	fmul	R5[ebp]			;; R65,R56,R66,R55,R4,R3
	fxch	st(2)			;; R66,R56,R65,R55,R4,R3
	fsubp	st(3), st		;; R56,R65,R5,R4,R3
	fld	R7[ebx]
	fmul	R7[ebp]			;; R77,R56,R65,R5,R4,R3
	fxch	st(1)			;; R56,R77,R65,R5,R4,R3
	faddp	st(2), st		;; R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R8[ebp]			;; R88,R77,R6,R5,R4,R3
	fld	R7[ebx]
	fmul	R8[ebp]			;; R78,R88,R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R7[ebp]			;; R87,R78,R88,R77,R6,R5,R4,R3
	fxch	st(2)			;; R88,R78,R87,R77,R6,R5,R4,R3
	fsubp	st(3), st		;; R78,R87,R7,R6,R5,R4,R3
	fld	R1[ebx]
	fmul	R1[ebp]			;; R1,R78,R87,R7,R6,R5,R4,R3
	fxch	st(1)			;; R78,R1,R87,R7,R6,R5,R4,R3
	faddp	st(2), st		;; R1,R8,R7,R6,R5,R4,R3
	fld	R2[ebx]
	fmul	R2[ebp]			;; R2,R1,R8,R7,R6,R5,R4,R3
	fxch	st(3)			;; R7,R1,R8,R2,R6,R5,R4,R3
	fsub	st(5), st		;; R5 = R5 - R7 (new R6)
	fadd	st, st			;; R7 = R7 * 2
	fxch	st(1)			;; R1,R7,R8,R2,R6,R5,R4,R3
	fst	QWORD PTR [esi-16]	;; Save product of sum of FFT values
	fsub	st, st(3)		;; R1 = R1 - R2 (new R2)
	fxch	st(2)			;; R8,R7,R1,R2,R6,R5,R4,R3
	fsub	st(4), st		;; R6 = R6 - R8 (new R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(2)			;; R1,R7,R8,R2,R6,R5,R4,R3
	fmul	HALF			;; Mul R1 by HALF
	fxch	st(5)			;; R5,R7,R8,R2,R6,R1,R4,R3
	fadd	st(1), st		;; R7 = R5 + R7 (new R5)
	fxch	st(4)			;; R6,R7,R8,R2,R5,R1,R4,R3
	fadd	st(2), st		;; R8 = R6 + R8 (new R7)
	fxch	st(5)			;; R1,R7,R8,R2,R5,R6,R4,R3
	fadd	st(3), st		;; R2 = R1 + R2 (new R1)
					;; R2,R5,R7,R1,R6,R8,R4,R3
	fxch	st(4)			;; R6,R5,R7,R1,R2,R8,R4,R3
	fsub	st(5), st		;; R8 = R8 - R6
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(6)			;; R4,R5,R7,R1,R2,R8,R6,R3
	fsub	st(4), st		;; R2 = R2 - R4 (new R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(5)			;; R8,R5,R7,R1,R2,R4,R6,R3
	fadd	st(6), st		;; R6 = R6 + R8
	fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	fxch	st(7)			;; R3,R5,R7,R1,R2,R4,R6,R8
	fsub	st(3), st		;; R1 = R1 - R3 (new R3)
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(6)			;; R6,R5,R7,R1,R2,R4,R3,R8
	fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
	fxch	st(4)			;; R2,R5,R7,R1,R6,R4,R3,R8
	fadd	st(5), st		;; R4 = R2 + R4 (new R2)
	fxch	st(3)			;; R1,R5,R7,R2,R6,R4,R3,R8
	fadd	st(6), st		;; R3 = R1 + R3 (new R1)
					;; R3,R5,R7,R4,R6,R2,R1,R8
	fstp	R3
	fstp	R5
	fstp	R7
	fstp	R4
	fstp	R6
	fstp	R2
	fstp	R1
	fstp	R8
	ENDM

eight_reals_with_mulf_3 MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R3[ebx]
	fmul	R3[ebp]			;; R33
	fld	R4[ebx]
	fmul	R4[ebp]			;; R44
	fld	R3[ebx]
	fmul	R4[ebp]			;; R34
	fld	R4[ebx]
	fmul	R3[ebp]			;; R43
	fxch	st(2)			;; R44,R34,R43,R33
	fsubp	st(3), st		;; R34,R43,R3
	fld	R5[ebx]
	fmul	R5[ebp]			;; R55,R34,R43,R3
	fxch	st(1)			;; R34,R55,R43,R3
	faddp	st(2), st		;; R55,R4,R3
	fld	R6[ebx]
	fmul	R6[ebp]			;; R66,R55,R4,R3
	fld	R5[ebx]
	fmul	R6[ebp]			;; R56,R66,R55,R4,R3
	fld	R6[ebx]
	fmul	R5[ebp]			;; R65,R56,R66,R55,R4,R3
	fxch	st(2)			;; R66,R56,R65,R55,R4,R3
	fsubp	st(3), st		;; R56,R65,R5,R4,R3
	fld	R7[ebx]
	fmul	R7[ebp]			;; R77,R56,R65,R5,R4,R3
	fxch	st(1)			;; R56,R77,R65,R5,R4,R3
	faddp	st(2), st		;; R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R8[ebp]			;; R88,R77,R6,R5,R4,R3
	fld	R7[ebx]
	fmul	R8[ebp]			;; R78,R88,R77,R6,R5,R4,R3
	fld	R8[ebx]
	fmul	R7[ebp]			;; R87,R78,R88,R77,R6,R5,R4,R3
	fxch	st(2)			;; R88,R78,R87,R77,R6,R5,R4,R3
	fsubp	st(3), st		;; R78,R87,R7,R6,R5,R4,R3
	fld	R1[ebx]
	fmul	R1[ebp]			;; R1,R78,R87,R7,R6,R5,R4,R3
	fxch	st(1)			;; R78,R1,R87,R7,R6,R5,R4,R3
	faddp	st(2), st		;; R1,R8,R7,R6,R5,R4,R3
	fld	R2[ebx]
	fmul	R2[ebp]			;; R2,R1,R8,R7,R6,R5,R4,R3
	fxch	st(3)			;; R7,R1,R8,R2,R6,R5,R4,R3
	fsub	st(5), st		;; R5 = R5 - R7 (new R6)
	fadd	st, st			;; R7 = R7 * 2
	fxch	st(1)			;; R1,R7,R8,R2,R6,R5,R4,R3
	fst	QWORD PTR [esi-16]	;; Save product of sum of FFT values
	fsub	st, st(3)		;; R1 = R1 - R2 (new R2)
	fxch	st(2)			;; R8,R7,R1,R2,R6,R5,R4,R3
	fsub	st(4), st		;; R6 = R6 - R8 (new R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(2)			;; R1,R7,R8,R2,R6,R5,R4,R3
	fmul	HALF			;; Mul R1 by HALF
	fxch	st(5)			;; R5,R7,R8,R2,R6,R1,R4,R3
	fadd	st(1), st		;; R7 = R5 + R7 (new R5)
	fxch	st(4)			;; R6,R7,R8,R2,R5,R1,R4,R3
	fadd	st(2), st		;; R8 = R6 + R8 (new R7)
	fxch	st(5)			;; R1,R7,R8,R2,R5,R6,R4,R3
	fadd	st(3), st		;; R2 = R1 + R2 (new R1)
					;; R2,R5,R7,R1,R6,R8,R4,R3
	fxch	st(4)			;; R6,R5,R7,R1,R2,R8,R4,R3
	fsub	st(5), st		;; R8 = R8 - R6
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(7)			;; R3,R5,R7,R1,R2,R8,R4,R6
	fsub	st(3), st		;; R1 = R1 - R3 (new R3)
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(5)			;; R8,R5,R7,R1,R2,R3,R4,R6
	fadd	st(7), st		;; R6 = R6 + R8
	fmul	SQRTHALF		;; R8 = R8 * square root of 1/2
	fxch	st(6)			;; R4,R5,R7,R1,R2,R3,R8,R6
	fsub	st(4), st		;; R2 = R2 - R4 (new R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(7)			;; R6,R5,R7,R1,R2,R3,R8,R4
	fmul	SQRTHALF		;; R6 = R6 * square root of 1/2
	fxch	st(3)			;; R1,R5,R7,R6,R2,R3,R8,R4
	fadd	st(5), st		;; R3 = R1 + R3 (new R1)
	fxch	st(4)			;; R2,R5,R7,R6,R1,R3,R8,R4
	fadd	st(7), st		;; R4 = R2 + R4 (new R2)
					;; R4,R5,R7,R6,R3,R1,R8,R2
	fxch	st(1)			;; R5,R4,R7,R6,R3,R1,R8,R2
	fsub	st(5), st		;; R1 = R1 - R5 (new R5)
	fadd	st, st			;; R5 = R5 * 2
	fxch	st(3)			;; R6,R4,R7,R5,R3,R1,R8,R2
	fsub	st(7), st		;; R2 = R2 - R6 (new R6)
	fadd	st, st			;; R6 = R6 * 2
	fxch	st(2)			;; R7,R4,R6,R5,R3,R1,R8,R2
	fsub	st(4), st		;; R3 = R3 - R7 (new R7)
	fadd	st, st			;; R7 = R7 * 2
	fxch	st(6)			;; R8,R4,R6,R5,R3,R1,R7,R2
	fsub	st(1), st		;; R4 = R4 - R8 (new R8)
	fadd	st, st			;; R8 = R8 * 2
	fxch	st(5)			;; R1,R4,R6,R5,R3,R8,R7,R2
	fadd	st(3), st		;; R5 = R1 + R5 (new R1)
	fxch	st(7)			;; R2,R4,R6,R5,R3,R8,R7,R1
	fadd	st(2), st		;; R6 = R2 + R6 (new R2)
	fxch	st(4)			;; R3,R4,R6,R5,R2,R8,R7,R1
	fadd	st(6), st		;; R7 = R3 + R7 (new R3)
	fxch	st(1)			;; R4,R3,R6,R5,R2,R8,R7,R1
	fadd	st(5), st		;; R8 = R4 + R8 (new R4)
					;; R8,R7,R2,R1,R6,R4,R3,R5
	fstp	R8
	fstp	R7
	fstp	R2
	fstp	R1
	fstp	R6
	fstp	R4
	fstp	R3
	fstp	R5
	ENDM

four_complex_with_mult MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R2			;; R2
	fmul	QWORD PTR [edi]		;; A2 = R2 * sine
	fld	R6			;; I2,A2
	fmul	QWORD PTR [edi]		;; B2 = I2 * sine
	fld	R3			;; R3,B2,A2
	fmul	QWORD PTR [edi+16]	;; A3 = R3 * sine
	fld	st(1)			;; C2 = B2 (C2,A3,B2,A2)
	fmul	QWORD PTR [edi+8]	;; C2 = C2 * cosine/sine
	fld	R7			;; I3,C2,A3,B2,A2
	fmul	QWORD PTR [edi+16]	;; B3 = I3 * sine
	fxch	st(4)			;; A2,C2,A3,B2,B3
	fadd	st(1), st		;; C2 = C2 + A2 (new I2)
	fmul	QWORD PTR [edi+8]	;; A2 = A2 * cosine/sine
	fld	R8			;; I4,A2,C2,A3,B2,B3
	fmul	QWORD PTR [edi+32]	;; B4 = I4 * sine
	fxch	st(4)			;; B2,A2,C2,A3,B4,B3
	fsubp	st(1), st		;; A2 = A2 - B2 (new R2)
	fld	R4			;; R4,A2,C2,A3,B4,B3
	fmul	QWORD PTR [edi+32]	;; A4 = R4 * sine
	fld	st(5)			;; C3 = B3 (C3,A4,A2,C2,A3,B4,B3)
	fmul	QWORD PTR [edi+24]	;; C3 = C3 * cosine/sine
	fld	st(5)			;; C4 = B4 (C4,C3,A4,A2,C2,A3,B4,B3)
	fmul	QWORD PTR [edi+40]	;; C4 = C4 * cosine/sine
	fxch	st(5)			;; A3,C3,A4,A2,C2,C4,B4,B3
	fadd	st(1), st		;; C3 = C3 + A3 (new I3)
	fmul	QWORD PTR [edi+24]	;; A3 = A3 * cosine/sine
	fxch	st(2)			;; A4,C3,A3,A2,C2,C4,B4,B3
	fadd	st(5), st		;; C4 = C4 + A4 (new I4)
	fmul	QWORD PTR [edi+40]	;; A4 = A4 * cosine/sine
	fxch	st(7)			;; B3,C3,A3,A2,C2,C4,B4,A4
	fsubp	st(2), st		;; A3 = A3 - B3 (new R3)
	fld	R5			;; I1,C3,A3,A2,C2,C4,B4,A4
	fxch	st(6)			;; B4,C3,A3,A2,C2,C4,I1,A4
	fsubp	st(7), st		;; A4 = A4 - B4 (new R4)
	fld	R1			;; R1,I3,R3,R2,I2,I4,I1,R4

	fxch	st(2)			;; R3,I3,R1,R2,I2,I4,I1,R4
	fsub	st(2), st		;; R1 = R1 - R3 (new R3)
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(1)			;; I3,R3,R1,R2,I2,I4,I1,R4
	fsub	st(6), st		;; I1 = I1 - I3 (new I3)
	fadd	st, st			;; I3 = I3 * 2
	fxch	st(7)			;; R4,R3,R1,R2,I2,I4,I1,I3
	fsub	st(3), st		;; R2 = R2 - R4 (new R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(5)			;; I4,R3,R1,R2,I2,R4,I1,I3
	fsub	st(4), st		;; I2 = I2 - I4 (new I4)
	fadd	st, st			;; I4 = I4 * 2
	fxch	st(2)			;; R1,R3,I4,R2,I2,R4,I1,I3
	fadd	st(1), st		;; R3 = R1 + R3 (new R1)
	fxch	st(3)			;; R2,R3,I4,R1,I2,R4,I1,I3
	fadd	st(5), st		;; R4 = R2 + R4 (new R2)
	fxch	st(6)			;; I1,R3,I4,R1,I2,R4,R2,I3
	fadd	st(7), st		;; I3 = I1 + I3 (new I1)
 	fxch	st(4)			;; I2,R3,I4,R1,I1,R4,R2,I3
	fadd	st(2), st		;; I4 = I2 + I4 (new I2)
					;; I4,R1,I2,R3,I3,R2,R4,I1
	 fxch	st(5)			;; R2,R1,I2,R3,I3,I4,R4,I1
	 fsub	st(1), st		;; R1 = R1 - R2 (new R2)
	 fadd	st, st			;; R2 = R2 * 2
	 fxch	st(2)			;; I2,R1,R2,R3,I3,I4,R4,I1
	 fsub	st(7), st		;; I1 = I1 - I2 (new I2)
	 fadd	st, st			;; I2 = I2 * 2
	 fxch	st(1)			;; R1,I2,R2,R3,I3,I4,R4,I1
	 fst	R3			;; Save new R2
	 faddp	st(2), st		;; R2 = R1 + R2 (new R1)
	 fxch	st(6)			;; I1,newR1,R3,I3,I4,R4,I2
	 fst	R4			;; Save new I2
	 faddp	st(6), st		;; I2 = I1 + I2 (new I1)
	 				;; newR1,R3,I3,I4,R4,newI1
	fld	R1[ebp]
	fmul	st, st(1)		;; R1R1,newR1,R3,I3,I4,R4,newI1
	fld	R2[ebp]
	fmul	st, st(7)		;; I1I1,R1R1,newR1,R3,I3,I4,R4,newI1
	fxch	st(5)			;; I4,R1R1,newR1,R3,I3,I1I1,R4,newI1
	 fsub	st(3), st		;; R3 = R3 - I4 (new R3)
	 fxch	st(2)			;; newR1,R1R1,I4,R3,I3,I1I1,R4,newI1
	fmul	R2[ebp]			;; R1I1,R1R1,I4,R3,I3,I1I1,R4,newI1
	fxch	st(2)			;; I4,R1R1,R1I1,R3,I3,I1I1,R4,newI1
	 fadd	st, st			;; I4 = I4 * 2
	 fxch	st(7)			;; newI1,R1R1,R1I1,R3,I3,I1I1,R4,I4
	fmul	R1[ebp]			;; I1R1,R1R1,R1I1,R3,I3,I1I1,R4,I4
	fxch	st(5)			;; I1I1,R1R1,R1I1,R3,I3,I1R1,R4,I4
	fsubp	st(1), st		;; R1 = R1R1 - I1I1
	fld	R5[ebp]	
	fmul	st, st(3)		;; R3R3,R1,R1I1,R3,I3,I1R1,R4,I4
	fxch	st(3)			;; R3,R1,R1I1,R3R3,I3,I1R1,R4,I4
	 fadd	st(7), st		;; I4 = R3 + I4 (new R4)
	fmul	R6[ebp]			;; R3I3,R1,R1I1,R3R3,I3,I1R1,R4,newR4
	fxch	st(6)			;; R4,R1,R1I1,R3R3,I3,I1R1,R3I3,newR4
	 fsub	st(4), st		;; I3 = I3 - R4 (new I4)
	 fadd	st, st			;; R4 = R4 * 2
	 fxch	st(1)			;; R1,R4,R1I1,R3R3,I3,I1R1,R3I3,newR4
	 fstp	R1			;; R4,R1I1,R3R3,I3,I1R1,R3I3,newR4
	fld	R7[ebp]
	fmul	st, st(7)		;; R4R4,R4,R1I1,R3R3,I3,I1R1,R3I3,newR4
	fxch	st(5)			;; I1R1,R4,R1I1,R3R3,I3,R4R4,R3I3,newR4
	faddp	st(2), st		;; I1 = R1I1 + I1R1
	fxch	st(6)			;; newR4,I1,R3R3,I3,R4R4,R3I3,R4
	fmul	R8[ebp]			;; R4I4,I1,R3R3,I3,R4R4,R3I3,R4
	fld	R8[ebp]
	fmul	st, st(4)		;; I4I4,R4I4,I1,R3R3,I3,R4R4,R3I3,R4
	fxch	st(4)			;; I3,R4I4,I1,R3R3,I4I4,R4R4,R3I3,R4
	 fadd	st(7), st		;; R4 = I3 + R4 (new I3)
	fmul	R7[ebp]		      ;; I4R4,R4I4,I1,R3R3,I4I4,R4R4,R3I3,newI3
	fxch	st(4)		      ;; I4I4,R4I4,I1,R3R3,I4R4,R4R4,R3I3,newI3
	fsubp	st(5), st		;; R4 = R4R4 - I4I4
	fld	R6[ebp]
	fmul	st, st(7)		;; I3I3,R4I4,I1,R3R3,I4R4,R4,R3I3,newI3
	fxch	st(4)			;; I4R4,R4I4,I1,R3R3,I3I3,R4,R3I3,newI3
	faddp	st(1), st		;; I4 = R4I4 + I4R4
	fld	R3[ebp]
	fmul	R3			;; R2R2,I4,I1,R3R3,I3I3,R4,R3I3,newI3
	fxch	st(4)			;; I3I3,I4,I1,R3R3,R2R2,R4,R3I3,newI3
	fsubp	st(3), st		;; R3 = R3R3 - I3I3
	fxch	st(6)			;; newI3,I1,R3,R2R2,R4,R3I3,I4
	fmul	R5[ebp]			;; I3R3
	fld	R4[ebp]
	fmul	R4			;; I2I2,I3R3,I1,R3,R2R2,R4,R3I3,I4
	fxch	st(1)			;; I3R3,I2I2,I1,R3,R2R2,R4,R3I3,I4
	faddp	st(6), st		;; I3 = R3I3 + I3R3
	fld	R3
	fmul	R4[ebp]			;; R2I2,I2I2,I1,R3,R2R2,R4,I3,I4
	fxch	st(1)			;; I2I2,R2I2,I1,R3,R2R2,R4,I3,I4
	fsubp	st(4), st		;; R2 = R2R2 - I2I2
	fld	R4
	fmul	R3[ebp]			;; I2R2,R2I2,I1,R3,R2,R4,I3,I4
	fxch	st(7)			;; I4,R2I2,I1,R3,R2,R4,I3,I2R2
	 fsub	st(6), st		;; I3 = I3 - I4 (new R4)
	 fadd	st, st			;; I4 = I4 * 2
	 fxch	st(7)			;; I2R2,R2I2,I1,R3,R2,R4,I3,I4
	faddp	st(1), st		;; I2 = R2I2 + I2R2
	 fld	R1			;; R1,I2,I1,R3,R2,R4,I3,I4
	 fsub	st, st(4)		;; R1 = R1 - R2 (new R2)
	 fxch	st(1)			;; I2,R1,I1,R3,R2,R4,I3,I4
	 fsub	st(2), st		;; I1 = I1 - I2 (new I2)
	 fadd	st, st			;; I2 = I2 * 2
	 fxch	st(3)			;; R3,R1,I1,I2,R2,R4,I3,I4
	 fsub	st(5), st		;; R4 = R4 - R3 (new I4)
	 fadd	st, st			;; R3 = R3 * 2
	 fxch	st(6)			;; I3,R1,I1,I2,R2,R4,R3,I4
	 fadd	st(7), st		;; I4 = I3 + I4 (new I3)
	 fxch	st(4)			;; R2,R1,I1,I2,I3,R4,R3,I4
	 fadd	R1			;; R2 = R1 + R2 (new R1)
	 fxch	st(2)			;; I1,R1,R2,I2,I3,R4,R3,I4
	 fadd	st(3), st		;; I2 = I1 + I2 (new I1)
	 fxch	st(5)			;; R4,R1,R2,I2,I3,I1,R3,I4
	 fadd	st(6), st		;; R3 = R3 + R4 (new R3)
					;; I4,R2,R1,I1,R4,I2,R3,I3
	fxch	st(4)			;; R4,R2,R1,I1,I4,I2,R3,I3
	fsub	st(1), st		;; R2 = R2 - R4 (new R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(4)			;; I4,R2,R1,I1,R4,I2,R3,I3
	fsub	st(5), st		;; I2 = I2 - I4 (new I4)
	fadd	st, st			;; I4 = I4 * 2
	fxch	st(2)			;; R1,R2,I4,I1,R4,I2,R3,I3
	fsub	st, st(6)		;; R1 = R1 - R3 (new R3)
	fxch	st(6)			;; R3,R2,I4,I1,R4,I2,R1,I3
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(7)			;; I3,R2,I4,I1,R4,I2,R1,R3
	fsub	st(3), st		;; I1 = I1 - I3 (new I3)
	fadd	st, st			;; I3 = I3 * 2
	fxch	st(1)			;; R2,I3,I4,I1,R4,I2,R1,R3
	fadd	st(4), st		;; R4 = R2 + R4 (new R2)
	fmul	QWORD PTR [edi+32]	;; A4 = new R4 * sine
	fxch	st(5)			;; I2,I3,I4,I1,R4,A4,R1,R3
	fadd	st(2), st		;; I4 = I2 + I4 (new I2)
	fmul	QWORD PTR [edi+32]	;; B4 = new I4 * sine
	fxch	st(6)			;; R1,I3,I4,I1,R4,A4,B4,R3
	fadd	st(7), st		;; R3 = R1 + R3 (new R1)
	fmul	QWORD PTR [edi+16]	;; A3 = new R3 * sine
	fxch	st(3)			;; I1,I3,I4,A3,R4,A4,B4,R3
	fadd	st(1), st		;; I3 = I1 + I3 (new I1)
	fmul	QWORD PTR [edi+16]	;; B3 = new I3 * sine
					;; B3,I1,I2,A3,R2,A4,B4,R1
	fxch	st(7)			;; R1,I1,I2,A3,R2,A4,B4,B3
	fstp	R1			;; I1,I2,A3,R2,A4,B4,B3
	fstp	R5			;; I2,A3,R2,A4,B4,B3
	fmul	QWORD PTR [edi]		;; B2 = I2 * sine
	fld	st(5)			;; C3 = B3 (C3,B2,A3,R2,A4,B4,B3)
	fmul	QWORD PTR [edi+24]	;; C3 = C3 * cosine/sine
	fld	st(5)			;; C4 = B4 (C4,C3,B2,A3,R2,A4,B4,B3)
	fmul	QWORD PTR [edi+40]	;; C4 = C4 * cosine/sine
	fxch	st(3)			;; A3,C3,B2,C4,R2,A4,B4,B3
	fsub	st(1), st		;; C3 = C3 - A3 (new I3)
	fmul	QWORD PTR [edi+24]	;; A3 = A3 * cosine/sine
	fxch	st(5)			;; A4,I3,B2,C4,R2,A3,B4,B3
	fsub	st(3), st		;; C4 = C4 - A4 (new I4)
	fmul	QWORD PTR [edi+40]	;; A4 = A4 * cosine/sine
	fxch	st(5)			;; A3,I3,B2,I4,R2,A4,B4,B3
	faddp	st(7), st		;; B3 = B3 + A3 (new R3)
	fxch	st(3)			;; R2,B2,I4,I3,A4,B4,R3
	fmul	QWORD PTR [edi]		;; A2 = R2 * sine
	fld	st(1)			;; C2 = B2 (C2,A2,B2,I4,I3,A4,B4,R3)
	fmul	QWORD PTR [edi+8]	;; C2 = C2 * cosine/sine
	fxch	st(4)			;; I3,A2,B2,I4,C2,A4,B4,R3
	fstp	R7			;; A2,B2,I4,C2,A4,B4,R3
	fsub	st(3), st		;; C2 = C2 - A2 (new I2)
	fmul	QWORD PTR [edi+8]	;; A2 = A2 * cosine/sine
	fxch	st(6)			;; R3,B2,I4,I2,A4,B4,A2
	fstp	R3			;; B2,I4,I2,A4,B4,A2
	faddp	st(5), st		;; A2 = B2 + A2 (new R2)
	fxch	st(2)			;; A4,I2,I4,B4,R2
	faddp	st(3), st		;; B4 = B4 + A4 (new R4)
					;; I2,I4,R4,R2
	fstp	R6
	fstp	R8
	fstp	R4
	fstp	R2
	ENDM

four_complex_with_mulf MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1[ebx]
	fmul	R1[ebp]			;; R11
	fld	R2[ebx]
	fmul	R2[ebp]			;; R22,R11
	fld	R1[ebx]
	fmul	R2[ebp]			;; R12,R22,R11
	fld	R2[ebx]
	fmul	R1[ebp]			;; R21,R12,R22,R11
	fxch	st(2)			;; R22,R12,R21,R11
	fsubp	st(3), st		;; R12,R21,R1
	fld	R3[ebx]
	fmul	R3[ebp]			;; R33,R12,R21,R1
	fxch	st(1)			;; R12,R33,R21,R1
	faddp	st(2), st		;; R33,I1,R1
	fld	R4[ebx]
	fmul	R4[ebp]			;; R44,R33,I1,R1
	fld	R3[ebx]
	fmul	R4[ebp]			;; R34,R44,R33,I1,R1
	fld	R4[ebx]
	fmul	R3[ebp]			;; R43,R34,R44,R33,I1,R1
	fxch	st(2)			;; R44,R34,R43,R33,I1,R1
	fsubp	st(3), st		;; R34,R43,R2,I1,R1
	fxch	st(3)			;; I1,R43,R2,R34,R1
	fstp	R2			;; R43,R2,R34,R1
	fld	R5[ebx]
	fmul	R5[ebp]			;; R55,R43,R2,R34,R1
	fxch	st(1)			;; R43,R55,R2,R34,R1
	faddp	st(3), st		;; R55,R2,I2,R1
	fld	R6[ebx]
	fmul	R6[ebp]			;; R66,R55,R2,I2,R1
	fld	R5[ebx]
	fmul	R6[ebp]			;; R56,R66,R55,R2,I2,R1
	fld	R6[ebx]
	fmul	R5[ebp]			;; R65,R56,R66,R55,R2,I2,R1
	fxch	st(2)			;; R66,R56,R65,R55,R2,I2,R1
	fsubp	st(3), st		;; R56,R65,R3,R2,I2,R1
	fld	R7[ebx]
	fmul	R7[ebp]			;; R77,R56,R65,R3,R2,I2,R1
	fxch	st(1)			;; R56,R77,R65,R3,R2,I2,R1
	faddp	st(2), st		;; R77,I3,R3,R2,I2,R1
	fld	R8[ebx]
	fmul	R8[ebp]			;; R88,R77,I3,R3,R2,I2,R1
	fld	R7[ebx]
	fmul	R8[ebp]			;; R78,R88,R77,I3,R3,R2,I2,R1
	fxch	st(1)			;; R88,R78,R77,I3,R3,R2,I2,R1
	fsubp	st(2), st		;; R78,R4,I3,R3,R2,I2,R1
	fld	R8[ebx]
	fmul	R7[ebp]			;; R87,R78,R4,I3,R3,R2,I2,R1
	fxch	st(5)			;; R2,R78,R4,I3,R3,R87,I2,R1
	 fsub	st(7), st		;; R1 = R1 - R2 (new R2)
	 fadd	st, st			;; R2 = R2 * 2
	 fxch	st(5)			;; R87,R78,R4,I3,R3,R2,I2,R1
	faddp	st(1), st		;; I4,R4,I3,R3,R2,I2,R1
	 fld	R2			;; I1,I4,R4,I3,R3,R2,I2,R1
	 fsub	st, st(6)		;; I1 = I1 - I2 (new I2)
	 fxch	st(4)			;; R3,I4,R4,I3,I1,R2,I2,R1
	 fsub	st(2), st		;; R4 = R4 - R3 (new I4)
	 fadd	st, st			;; R3 = R3 * 2
	 fxch	st(1)			;; I4,R3,R4,I3,I1,R2,I2,R1
	 fsub	st(3), st		;; I3 = I3 - I4 (new R4)
	 fadd	st, st			;; I4 = I4 * 2
	 fxch	st(7)			;; R1,R3,R4,I3,I1,R2,I2,I4
	 fadd	st(5), st		;; R2 = R1 + R2 (new R1)
	 fxch	st(6)			;; I2,R3,R4,I3,I1,R2,R1,I4
	 fadd	R2			;; I2 = I1 + I2 (new I1)
	 fxch	st(2)			;; R4,R3,I2,I3,I1,R2,R1,I4
	 fadd	st(1), st		;; R3 = R3 + R4 (new R3)
	 fxch	st(3)			;; I3,R3,I2,R4,I1,R2,R1,I4
	 fadd	st(7), st		;; I4 = I3 + I4 (new I3)
					;; R4,R3,I1,I4,I2,R1,R2,I3
	fsub	st(6), st		;; R2 = R2 - R4 (new R4)
	fadd	st, st			;; R4 = R4 * 2
	fxch	st(3)			;; I4,R3,I1,R4,I2,R1,R2,I3
	fsub	st(4), st		;; I2 = I2 - I4 (new I4)
	fadd	st, st			;; I4 = I4 * 2
	fxch	st(1)			;; R3,I4,I1,R4,I2,R1,R2,I3
	fsub	st(5), st		;; R1 = R1 - R3 (new R3)
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(7)			;; I3,I4,I1,R4,I2,R1,R2,R3
	fsub	st(2), st		;; I1 = I1 - I3 (new I3)
	fadd	st, st			;; I3 = I3 * 2
	fxch	st(6)			;; R2,I4,I1,R4,I2,R1,I3,R3
	fadd	st(3), st		;; R4 = R2 + R4 (new R2)
	fmul	QWORD PTR [edi+32]	;; A4 = new R4 * sine
	fxch	st(4)			;; I2,I4,I1,R4,A4,R1,I3,R3
	fadd	st(1), st		;; I4 = I2 + I4 (new I2)
	fmul	QWORD PTR [edi+32]	;; B4 = new I4 * sine
	fxch	st(5)			;; R1,I4,I1,R4,A4,B4,I3,R3
	fadd	st(7), st		;; R3 = R1 + R3 (new R1)
	fmul	QWORD PTR [edi+16]	;; A3 = new R3 * sine
	fxch	st(2)			;; I1,I4,A3,R4,A4,B4,I3,R3
	fadd	st(6), st		;; I3 = I1 + I3 (new I1)
	fmul	QWORD PTR [edi+16]	;; B3 = new I3 * sine
					;; B3,I2,A3,R2,A4,B4,I1,R1

	fxch	st(7)			;; R1,I2,A3,R2,A4,B4,I1,B3
	fstp	R1			;; I2,A3,R2,A4,B4,I1,B3
	fmul	QWORD PTR [edi]		;; B2 = I2 * sine
	fxch	st(5)			;; I1,A3,R2,A4,B4,B2,B3
	fstp	R5			;; A3,R2,A4,B4,B2,B3
	fld	st(5)			;; C3 = B3 (C3,A3,R2,A4,B4,B2,B3)
	fmul	QWORD PTR [edi+24]	;; C3 = C3 * cosine/sine
	fld	st(4)			;; C4 = B4 (C4,C3,A3,R2,A4,B4,B2,B3)
	fmul	QWORD PTR [edi+40]	;; C4 = C4 * cosine/sine
	fxch	st(2)			;; A3,C3,C4,R2,A4,B4,B2,B3
	fsub	st(1), st		;; C3 = C3 - A3 (new I3)
	fmul	QWORD PTR [edi+24]	;; A3 = A3 * cosine/sine
	fxch	st(4)			;; A4,C3,C4,R2,A3,B4,B2,B3
	fsub	st(2), st		;; C4 = C4 - A4 (new I4)
	fmul	QWORD PTR [edi+40]	;; A4 = A4 * cosine/sine
	fxch	st(4)			;; A3,C3,C4,R2,A4,B4,B2,B3
	faddp	st(7), st		;; B3 = B3 + A3 (new R3)
	fxch	st(2)			;; R2,C4,C3,A4,B4,B2,B3
	fmul	QWORD PTR [edi]		;; A2 = R2 * sine
	fld	st(5)			;; C2 = B2 (C2,A2,C4,C3,A4,B4,B2,B3)
	fmul	QWORD PTR [edi+8]	;; C2 = C2 * cosine/sine
	fxch	st(3)			;; C3,A2,C4,C2,A4,B4,B2,B3
	fstp	R7			;; A2,C4,C2,A4,B4,B2,B3
	fsub	st(2), st		;; C2 = C2 - A2 (new I2)
	fmul	QWORD PTR [edi+8]	;; A2 = A2 * cosine/sine
	fxch	st(1)			;; C4,A2,C2,A4,B4,B2,B3
	fstp	R8			;; A2,C2,A4,B4,B2,B3
	faddp	st(4), st		;; B2 = B2 + A2 (new R2)
	fstp	R6			;; A4,B4,B2,B3
	faddp	st(1), st		;; B4 = B4 + A4 (new R4)
					;; R4,R2,R3
	fxch	st(2)			;; R3,R2,R4
	fstp	R3
	fstp	R2
	fstp	R4
	ENDM

two_two_complex_with_mult MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R4			;; I2
	fmul	QWORD PTR [edi]		;; B2 = I2 * sine
	fld	R2			;; R2,B2
	fmul	QWORD PTR [edi]		;; A2 = R2 * sine
	fld	R8			;; I4,A2,B2
	fmul	QWORD PTR [edi]		;; B4 = I4 * sine
	fld	R6			;; R4,B4,A2,B2
	fmul	QWORD PTR [edi]		;; A4 = R4 * sine
	fld	st(3)			;; C2 = B2 (C2,A4,B4,A2,B2)
	fmul	QWORD PTR [edi+8]	;; C2 = C2 * cosine/sine
	fld	st(2)			;; C4 = B4 (C4,C2,A4,B4,A2,B2)
	fmul	QWORD PTR [edi+8]	;; C4 = C4 * cosine/sine
	fxch	st(4)			;; A2,C2,A4,B4,C4,B2
	fadd	st(1), st		;; C2 = C2 + A2 (new I2)
	fmul	QWORD PTR [edi+8]	;; A2 = A2 * cosine/sine
	fxch	st(2)			;; A4,C2,A2,B4,C4,B2
	fadd	st(4), st		;; C4 = C4 + A4 (new negR4)
	fmul	QWORD PTR [edi+8]	;; A4 = A4 * cosine/sine
	fxch	st(5)			;; B2,C2,A2,B4,C4,A4
	fsubp	st(2), st		;; A2 = A2 - B2 (new R2)
	fld	R1			;; R1,C2,A2,B4,C4,A4
	fld	R3			;; I1,R1,C2,A2,B4,C4,A4
	fxch	st(4)			;; B4,R1,C2,A2,I1,C4,A4
	fsubp	st(6), st		;; A4 = A4 - B4 (new I4)
					;; R1,I2,R2,I1,negR4,I4
	fxch	st(2)			;; R2,I2,R1,I1,negR4,I4
	fsub	st(2), st		;; R1 = R1 - R2 (new R2)
	fadd	R1			;; R2 = R1 + R2 (new R1)
	fxch	st(1)			;; I2,R2,R1,I1,negR4,I4
	fsub	st(3), st		;; I1 = I1 - I2 (new I2)
	fadd	R3			;; I2 = I1 + I2 (new I1)
					;; I1,R1,R2,I2,negR4,I4
	fld	R7			;; I3,I1,R1,R2,I2,negR4,I4
	fxch	st(6)			;; I4,I1,R1,R2,I2,negR4,I3
	fsub	st(6), st		;; I3 = I3 - I4 (new I4)
	fadd	R7			;; I4 = I3 + I4 (new I3)
	fld	R5			;; R3,I4,I1,R1,R2,I2,negR4,I3
	fxch	st(6)			;; negR4,I4,I1,R1,R2,I2,R3,I3
	fsub	st(6), st		;; R3 = R3 + R4 (new R3)
	fadd	R5			;; R4 = R3 - R4 (new R4)
					;; R4,I3,I1,R1,R2,I2,R3,I4
	fxch	st(5)			;; I2,I3,I1,R1,R2,R4,R3,I4
	fstp	R4
	fstp	R6
	fstp	R2
	fstp	R1
	fstp	R3
	fstp	R7
	fstp	R5
	fstp	R8

	two_two_complex_with_mulf R1,R2,R3,R4,R5,R6,R7,R8
	ENDM

two_two_complex_with_mulf MACRO R1,R2,R3,R4,R5,R6,R7,R8
	fld	R1[ebx]
	fmul	R1[ebp]			;; R1*R1
	fld	R2[ebx]
	fmul	R2[ebp]			;; I1*I1
	fld	R3[ebx]
	fmul	R3[ebp]			;; R2*R2
	fld	R4[ebx]
	fmul	R4[ebp]			;; I2*I2
	fld	R1[ebx]
	fmul	R2[ebp]			;; R1*I1
	fld	R2[ebx]
	fmul	R1[ebp]			;; I1*R1
	fld	R3[ebx]
	fmul	R4[ebp]			;; R2*I2
	fld	R4[ebx]
	fmul	R3[ebp]			;; I2*R2
	fxch	st(6)			;; I1*I1,R2*I2,I1*R1,R1*I1,I2*I2,
					;;		R2*R2,I2*R2,R1*R1
	fsubp	st(7), st		;; R1*R1 - I1*I1 (new R1)
	fxch	st(3)			;; I2*I2,I1*R1,R1*I1,R2*I2,R2*R2,
					;;			I2*R2,R1
	fsubp	st(4), st		;; R2*R2 - I2*I2 (new R2)
					;; I1*R1,R1*I1,R2*I2,R2,I2*R2,R1
	faddp	st(1), st		;; R1*I1 + I1*R1 (new I1)
	fxch	st(3)			;; I2*R2,R2*I2,R2,I1,R1
	faddp	st(1), st		;; R2*I2 + I2*R2 (new I2)
	fxch	st(1)			;; R2,I2,I1,R1
	fsub	st(3), st		;; R1 = R1 - R2 (new R2)
	fadd	st, st			;; R2 = R2 * 2
	fxch	st(1)			;; I2,R2,I1,R1
	fsub	st(2), st		;; I1 = I1 - I2 (new I2)
	fadd	st, st			;; I2 = I2 * 2
	fld	R5[ebx]
	fmul	R5[ebp]			;; R3*R3,I2,R2,I1,R1
	fxch	st(4)			;; R1,I2,R2,I1,R3*R3
	fadd	st(2), st		;; R2 = R1 + R2 (new R1)
	fxch	st(3)			;; I1,I2,R2,R1,R3*R3
	fadd	st(1), st		;; I2 = I1 + I2 (new I1)
					;; I2,I1,R1,R2,R3*R3
	fld	R6[ebx]
	fmul	R6[ebp]			;; I3*I3,I2,I1,R1,R2,R3*R3
	fxch	st(3)			;; R1,I2,I1,I3*I3,R2,R3*R3
	fstp	R1			;; Store R1
	fld	R7[ebx]
	fmul	R7[ebp]			;; R4*R4,I2,I1,I3*I3,R2,R3*R3
	fxch	st(2)			;; I1,I2,R4*R4,I3*I3,R2,R3*R3
	fstp	R3			;; Store I1
	fld	R8[ebx]
	fmul	R8[ebp]			;; I4*I4,I2,R4*R4,I3*I3,R2,R3*R3
	fxch	st(3)			;; I3*I3,I2,R4*R4,I4*I4,R2,R3*R3
	fsubp	st(5), st		;; R3*R3 - I3*I3 (new R3)
	fld	R6[ebx]
	fmul	R5[ebp]			;; I3*R3,I2,R4*R4,I4*I4,R2,R3
	fxch	st(3)			;; I4*I4,I2,R4*R4,I3*R3,R2,R3
	fsubp	st(2), st		;; R4*R4 - I4*I4 (new R4)
	fld	R5[ebx]
	fmul	R6[ebp]			;; R3*I3,I2,R4,I3*R3,R2,R3
	fld	R7[ebx]
	fmul	R8[ebp]			;; R4*I4,R3*I3,I2,R4,I3*R3,R2,R3
	fld	R8[ebx]
	fmul	R7[ebp]			;; I4*R4,R4*I4,R3*I3,I2,R4,I3*R3,R2,R3
	fxch	st(2)			;; R3*I3,R4*I4,I4*R4,I2,R4,I3*R3,R2,R3
	faddp	st(5), st		;; R3*I3 + I3*R3 (new I3)
	fxch	st(5)			;; R2,I4*R4,I2,R4,I3,R4*I4,R3
	fmul	QWORD PTR [edi]		;; A2 = new R2 * sine
	fxch	st(5)			;; R4*I4,I4*R4,I2,R4,I3,A2,R3
	faddp	st(1), st		;; R4*I4 + I4*R4 (new I4)
	fxch	st(1)			;; I2,I4,R4,I3,A2,R3
	fmul	QWORD PTR [edi]		;; B2 = new I2 * sine
	fxch	st(5)			;; R3,I4,R4,I3,A2,B2
	fsub	st(2), st		;; R4 = R4 - R3 (new negR4)
	fadd	st, st			;; R3 = R3 * 2
	fxch	st(1)			;; I4,R3,R4,I3,A2,B2
	fsub	st(3), st		;; I3 = I3 - I4 (new I4)
	fadd	st, st			;; I4 = I4 * 2
	fxch	st(2)			;; R4,R3,I4,I3,A2,B2
	fadd	st(1), st		;; R3 = R3 + R4 (new R3)
	fmul	QWORD PTR [edi]		;; A4 = new negR4 * sine
	fxch	st(3)			;; I3,R3,I4,A4,A2,B2
	fadd	st(2), st		;; I4 = I3 + I4 (new I3)
	fmul	QWORD PTR [edi]		;; B4 = new I4 * sine
					;; B4,R3,I3,A4,A2,B2
	fld	st(5)			;; C2 = B2 (C2,B4,R3,I3,A4,A2,B2)
	fmul	QWORD PTR [edi+8]	;; C2 = C2 * cosine/sine
	fld	st(4)			;; C4 = A4 (C4,C2,B4,R3,I3,A4,A2,B2)
	fmul	QWORD PTR [edi+8]	;; C4 = C4 * cosine/sine
	fxch	st(6)			;; A2,C2,B4,R3,I3,A4,C4,B2
	fsub	st(1), st		;; C2 = C2 - A2 (new I2)
	fmul	QWORD PTR [edi+8]	;; A2 = A2 * cosine/sine
	fxch	st(2)			;; B4,C2,A2,R3,I3,A4,C4,B2
	fsub	st(6), st		;; C4 = C4 - B4 (new I4)
	fmul	QWORD PTR [edi+8]	;; B4 = B4 * cosine/sine
	fxch	st(2)			;; A2,C2,B4,R3,I3,A4,C4,B2
	faddp	st(7), st		;; B2 = A2 + B2 (new R2)
	fxch	st(2)			;; R3,B4,C2,I3,A4,C4,B2
	fstp	R5			;; R3
	faddp	st(3), st		;; A4 = A4 + B4 (new R4)
					;; C2,I3,A4,C4,B2
					;; I2,I3,R4,I4,R2
	fstp	R4
	fstp	R7
	fstp	R6
	fstp	R8
	fstp	R2
	ENDM

